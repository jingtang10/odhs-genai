{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPWWUAUPk/wHF5UkZLoGnt2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jingtang10/odhs-genai/blob/main/%5BODHS_D2_T1_05A_CL1%5D_Gemini_WHO_ANC_Guidelines_Eval_Data_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tD4U-FRgOykt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1tkM115zwBI"
      },
      "source": [
        "# WHO recommendations on antenatal care for a positive pregnancy experience\n",
        "\n",
        "\n",
        "### This notebook processes the WHO's recommendations on antenatal care, extracting key information to generate a collection of multiple-choice questions (MCQs) and short-answer questions aimed at fostering positive pregnancy experiences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwNjhAyUx5Vy"
      },
      "source": [
        "## Install the Gemini API SDK\n",
        "\n",
        "The Python SDK for the Gemini API is contained in the [google-generativeai package](https://pypi.org/project/google-generativeai/). Install the dependency using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGLP5DrSxtHG"
      },
      "outputs": [],
      "source": [
        "! pip install -U -q google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4jRgfsQx8d4"
      },
      "source": [
        "## Import the libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpT4Pp8Bx8Pz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import textwrap\n",
        "import pandas as pd\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from typing import Union, List\n",
        "\n",
        "from google.api_core import retry, exceptions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vChevak0zUh_"
      },
      "source": [
        "## Set up your API key\n",
        "\n",
        "\n",
        "To use the Gemini API, you'll need an API key. Store your API key in Colab Secrets named `GOOGLE_API_KEY`.   \n",
        "If you don't have an API key or need help creating a Colab Secrets, see the [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) guide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBzh7C86yAiq"
      },
      "outputs": [],
      "source": [
        "# passing the API key\n",
        "try:\n",
        "  from google.colab import userdata\n",
        "  GOOGLE_API_KEY = userdata.get ('GOOGLE_API_KEY')\n",
        "  genai.configure(api_key=GOOGLE_API_KEY)\n",
        "except ImportError:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I20XXYHeqf-"
      },
      "source": [
        "## Upload WHO's antenatal care guidelines file\n",
        "\n",
        "Use the [`upload_file`](https://ai.google.dev/gemini-api/docs/document-processing?lang=python#upload-document) API to temporarily store WHO's antenatal care guidelines pdf file. This process produces a file reference that can be used to prompt a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BuMQV37AX8M9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fc2eee4-8c44-4487-d723-1ae64367ea27"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5bb990d16d54>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdisplay_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"WHO recommendations on antenatal care for a positive pregnancy experience\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m pdf_file = genai.upload_file(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdisplay_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/generativeai/files.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(path, mime_type, name, display_name, resumable)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"files/{name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     response = client.create_file(\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmime_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmime_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresumable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresumable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/generativeai/client.py\u001b[0m in \u001b[0;36mcreate_file\u001b[0;34m(self, path, mime_type, name, display_name, resumable, metadata)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/file_service/client.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(self, request, name, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    924\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/file_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             response = getattr(self._session, method)(\n\u001b[0m\u001b[1;32m    497\u001b[0m                 \u001b[0;34m\"{host}{uri}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allow_redirects\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimeoutGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             response = super(AuthorizedSession, self).request(\n\u001b[0m\u001b[1;32m    542\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "file_path = \"9789241549912-eng.pdf\" #@param {type:\"string\"}\n",
        "display_name = \"WHO recommendations on antenatal care for a positive pregnancy experience\"\n",
        "\n",
        "pdf_file = genai.upload_file(\n",
        "    path=file_path,\n",
        "    display_name=display_name,\n",
        ")\n",
        "\n",
        "file_ref = genai.get_file(name=pdf_file.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvdVEtPP3cEw"
      },
      "source": [
        "## Select a suitable gemini model\n",
        "\n",
        "The Gemini API offers different models that are optimized for specific use cases. Here's a [brief overview of Gemini variants](https://ai.google.dev/gemini-api/docs/models/gemini?_gl=1*cyblbc*_up*MQ..&gclid=Cj0KCQjwsJO4BhDoARIsADDv4vB5i1gAcxplfDp37YCnHdYV1vFF_11JvdxwPjqBjujKpgMKrmDHM9caAlGLEALw_wcB) that are available.\n",
        "To ensure your prompts work correctly, check the input and output token limits. Make sure your document and desired output fit within these limits.\n",
        "\n",
        "We will be using [Gemini 1.5 pro](https://ai.google.dev/gemini-api/docs/models/gemini?_gl=1*cyblbc*_up*MQ..&gclid=Cj0KCQjwsJO4BhDoARIsADDv4vB5i1gAcxplfDp37YCnHdYV1vFF_11JvdxwPjqBjujKpgMKrmDHM9caAlGLEALw_wcB#gemini-1.5-pro) model in this tutorial to generate the questionnaires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHJA2k7BUE9c"
      },
      "outputs": [],
      "source": [
        "model_name = \"gemini-1.5-pro-latest\" #@param [\"gemini-1.5-pro-latest\", \"gemini-1.5-pro\", \"gemini-1.5-flash-latest\", \"gemini-1.5-flash\"]\n",
        "model_info = genai.get_model(f'models/{model_name}')\n",
        "\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Input Token Limit: {model_info.input_token_limit}\")\n",
        "print(f\"Output Token Limit: {model_info.output_token_limit}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uf4NjoOrWzW"
      },
      "source": [
        "## Let's define some utility functions to perform repetitative tasks:\n",
        "\n",
        "1.   Define the model\n",
        "2.   Call the model and generate text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEkPuLCyuUaC"
      },
      "outputs": [],
      "source": [
        "def model_def(model_name, safety_settings, tools=None):\n",
        "  model = genai.GenerativeModel(\n",
        "    model_name=model_name,\n",
        "    safety_settings=safety_settings,\n",
        "    tools=tools\n",
        "  )\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzaSSy9krcNH"
      },
      "outputs": [],
      "source": [
        "def format_items_for_prompt(items: list, item_type: str,model_name: str) -> str:\n",
        "  \"\"\"\n",
        "  Formats a list of items (questions, etc.) into a string for the prompt.\n",
        "  Handles different item types (e.g., \"mcq\", \"sqa\").\n",
        "  \"\"\"\n",
        "  formatted_text = \"\"\n",
        "  for item in items:\n",
        "    formatted_text += f\"Question: {item['Question']}\\n\"\n",
        "    formatted_text += f\"{item.get('Intervention', '')}\\n\"\n",
        "    if item_type == \"mcq\":\n",
        "      if model_name == 'gemini':\n",
        "        formatted_text += \"\".join(\n",
        "            [f\"{option}: {item[option]}\\n\" for option in [\"A\", \"B\", \"C\", \"D\", \"E\"]]\n",
        "        )\n",
        "        formatted_text += f\"Choice: {item.get('Choice', '')}\\n\\n\" # Include Choice if present\n",
        "      elif model_name == \"gemma\":\n",
        "          formatted_text += \"\".join(\n",
        "            [f\"{option}: {item[option]}\\n\" for option in [\"A\", \"B\", \"C\", \"D\", \"E\"]]\n",
        "        )\n",
        "          formatted_text += f\"Answer: \\n\"\n",
        "\n",
        "    elif item_type == \"sqa_eval\":\n",
        "      if model_name == 'gemini':\n",
        "        formatted_text += f\"Answer: {item.get('Answer', '')}\\n\\n\"\n",
        "      elif model_name == \"gemma\":\n",
        "        formatted_text += f\"Answer: {item.get('Answer_2b', '')}\\n\\n\"\n",
        "\n",
        "    elif item_type == \"sqa\":\n",
        "      if model_name == 'gemini':\n",
        "        formatted_text += f\"Answer: {item.get('Answer', '')}\\n\\n\"\n",
        "      elif model_name == 'gemma':\n",
        "        formatted_text += f\"Answer: \\n\\n\"\n",
        "\n",
        "  return formatted_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx3Uj_XNrdDA"
      },
      "outputs": [],
      "source": [
        "def call_model_and_extract(\n",
        "    chat_model: genai.GenerativeModel,\n",
        "    prompt: list,\n",
        "    function_name: str,\n",
        "    generation_config: dict = None,\n",
        "    tool_config: dict = None,\n",
        ") -> list:\n",
        "  \"\"\"Calls the language model and extracts the results from the function call.\"\"\"\n",
        "\n",
        "  try:\n",
        "    response = generate_text(\n",
        "        chat_model,\n",
        "        prompt,\n",
        "        generation_config=generation_config,\n",
        "        tool_config=tool_config,\n",
        "    )\n",
        "\n",
        "    if response.candidates[0].content.parts[0].function_call:\n",
        "      function_call = response.candidates[0].content.parts[0].function_call\n",
        "      extracted_results = type(function_call).to_dict(function_call)[\"args\"][\n",
        "          function_name\n",
        "      ]\n",
        "      return extracted_results\n",
        "    else:\n",
        "      return []  # Return empty list if no function call\n",
        "\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error calling model: {e}\")\n",
        "    return []  # Return empty list on error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbweT3poMPOS"
      },
      "source": [
        "## Generate text\n",
        "\n",
        "* The Gemini API's client library offers built-in retry mechanisms for handling transient errors.\n",
        "\n",
        "* The `generate_text` function sends a message to the chat model with the given prompt and\n",
        "  configurations, and returns the response. It includes retry logic to handle\n",
        "  transient errors.\n",
        "\n",
        "* For more info on error handling, take a look at the [error_handling quickstart](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Error_handling.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm0gVbVnPFoj"
      },
      "outputs": [],
      "source": [
        "@retry.Retry(\n",
        "    predicate=retry.if_transient_error,\n",
        "    initial=5,\n",
        "    maximum=10,\n",
        "    multiplier=2.0,\n",
        "    timeout=100,\n",
        ")\n",
        "\n",
        "def generate_text(\n",
        "    chat: genai.GenerativeModel,\n",
        "    prompt: Union[List[str], str],\n",
        "    generation_config: dict = None,\n",
        "    tool_config: dict = None\n",
        "):\n",
        "  \"\"\"Generates text using a chat model, with retry mechanism for transient errors.\n",
        "\n",
        "  This function sends a prompt to a chat model and returns the generated response.\n",
        "  It uses a retry decorator to handle transient errors, such as network issues,\n",
        "  allowing the function to automatically retry the operation multiple times\n",
        "  before giving up.\n",
        "\n",
        "  Args:\n",
        "    chat: The chat model object (an instance of `genai.Model`).\n",
        "    prompt: The text prompt to send to the chat model. Can be a string or a list of strings.\n",
        "    generation_config: (Optional) A dictionary containing configuration\n",
        "        parameters for the text generation process. This might include settings\n",
        "        like temperature, max tokens, etc. The specific format depends on the\n",
        "        `chat` object (genai.Model).  See GenAI's documentation for details.\n",
        "    tool_config: (Optional)  A dictionary containing configuration\n",
        "        parameters for any tools that the chat model might use. The specific\n",
        "        format depends on the `chat` object (genai.Model) and whether it\n",
        "        supports tools.\n",
        "\n",
        "  Returns:\n",
        "    The response from the chat model (genai.Response).\n",
        "\n",
        "\n",
        "  Raises:\n",
        "    retry.RetryError: If the function fails to generate text after multiple\n",
        "        retries due to persistent transient errors. The original exception\n",
        "        that triggered the retries will be chained to the `RetryError`.\n",
        "    Any other exception raised by `chat.generate_text`: If the `generate_text`\n",
        "        method raises an exception that is not considered a transient error,\n",
        "        the exception will be propagated directly without retries.\n",
        "  \"\"\"\n",
        "\n",
        "  response = chat.send_message(\n",
        "      prompt,\n",
        "      generation_config=generation_config,\n",
        "      tool_config=tool_config,\n",
        "  )\n",
        "  return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW1NdKELem4h"
      },
      "source": [
        "## Customize Safety Settings\n",
        "\n",
        "The Gemini API provides safety settings that you can adjust during the prototyping stage to determine if your application requires more or less restrictive safety configuration. You can adjust these settings across four filter categories to restrict or allow certain types of content.\n",
        "\n",
        "To make this customization you must define a safety_settings and pass it to model initialization as below.\n",
        "\n",
        "**Important:** To guarantee the Google commitment with the Responsible AI development and its [AI Principles](https://ai.google/responsibility/principles/), for some prompts Gemini will avoid generating the results even if you set all the filters to none."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6D4TSLQcGYR"
      },
      "outputs": [],
      "source": [
        "\n",
        "HARM_CATEGORY_DANGEROUS = \"BLOCK_NONE\" # @param [\"BLOCK_NONE\", \"BLOCK_ONLY_HIGH\", \"BLOCK_MEDIUM_AND_ABOVE\", \"BLOCK_LOW_AND_ABOVE\", \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\"]\n",
        "HARM_CATEGORY_HARASSMENT = \"BLOCK_NONE\" # @param [\"BLOCK_NONE\", \"BLOCK_ONLY_HIGH\", \"BLOCK_MEDIUM_AND_ABOVE\", \"BLOCK_LOW_AND_ABOVE\", \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\"]\n",
        "HARM_CATEGORY_HATE_SPEECH = \"BLOCK_NONE\" # @param [\"BLOCK_NONE\", \"BLOCK_ONLY_HIGH\", \"BLOCK_MEDIUM_AND_ABOVE\", \"BLOCK_LOW_AND_ABOVE\", \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\"]\n",
        "HARM_CATEGORY_SEXUALLY_EXPLICIT = \"BLOCK_NONE\" # @param [\"BLOCK_NONE\", \"BLOCK_ONLY_HIGH\", \"BLOCK_MEDIUM_AND_ABOVE\", \"BLOCK_LOW_AND_ABOVE\", \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\"]\n",
        "HARM_CATEGORY_DANGEROUS_CONTENT = \"BLOCK_NONE\" # @param [\"BLOCK_NONE\", \"BLOCK_ONLY_HIGH\", \"BLOCK_MEDIUM_AND_ABOVE\", \"BLOCK_LOW_AND_ABOVE\", \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\"]\n",
        "\n",
        "safety_settings = [\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_DANGEROUS\",\n",
        "        \"threshold\": HARM_CATEGORY_DANGEROUS,\n",
        "        },\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
        "        \"threshold\": HARM_CATEGORY_HARASSMENT,\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "        \"threshold\": HARM_CATEGORY_HATE_SPEECH,\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "        \"threshold\": HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "        \"threshold\": HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb9lbFsSlxdd"
      },
      "source": [
        "## Configure text generation\n",
        "\n",
        "\n",
        "Every prompt you send to the model includes [parameters]((https://ai.google.dev/gemini-api/docs/models/generative-models#model-parameters)) that control how the model generates responses. You can use [genai.GenerationConfig](https://ai.google.dev/api/generate-content#generationconfig) to configure these parameters. If you don't configure the parameters, the model uses default options, which can vary by model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNKXyNy00qpy"
      },
      "outputs": [],
      "source": [
        "temperature = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "top_p = 0.95 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "top_k = 64 #@param {type:\"integer\"}\n",
        "max_output_tokens = 8192 #@param {type:\"integer\"}\n",
        "\n",
        "generation_config ={\n",
        "    \"temperature\": 1,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 64,\n",
        "    \"max_output_tokens\": 8192,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wj1JbR30tbX"
      },
      "source": [
        "## Generate Multiple Choice Questions (MCQ's) for main 5 intervention types\n",
        "\n",
        "\n",
        "The questions created from the World Health Organization's guidelines are divided equally among the five main categories of recommendations for pregnancy care:\n",
        "\n",
        "1. Nutritional interventions\n",
        "2. Maternal and fetal assessment\n",
        "3. Preventive measures\n",
        "4. Interventions for common physiological symptoms\n",
        "5. Health systems interventions to improve the utilization and quality of antenatal care\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZPOI6zOrxWM"
      },
      "source": [
        "### Utility function for generating questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQdltukMrwUP"
      },
      "outputs": [],
      "source": [
        "def generate_questions(\n",
        "    chat_model: genai.GenerativeModel,\n",
        "    file_ref: genai.types.file_types.File,\n",
        "    base_prompt: str,\n",
        "    total_questions: int,\n",
        "    question_type: str,\n",
        "    question_type_function_name: str,\n",
        "    questions_per_prompt: int,\n",
        "    generation_config: dict = None,\n",
        "    tool_config: dict = None,\n",
        "    model_name: str = \"gemini\"\n",
        "):\n",
        "  \"\"\"\n",
        "  Generates multiple-choice or short-answer questions based on a given file\n",
        "  and prompt.\n",
        "\n",
        "  Args:\n",
        "    chat_model: `genai.GenerativeModel`. The ChatModel instance to use for\n",
        "      generation.\n",
        "    file_ref: `genai.types.file_types.File`. The File reference to use for\n",
        "      questions need to be generated.\n",
        "    base_prompt: str. The base prompt for generating questions.\n",
        "    total_questions: int. The total number of questions to generate.\n",
        "    question_type: str. The type of questions to generate. Can be either\n",
        "      \"mcq\" or \"sqa\".\n",
        "    questions_per_prompt: int. The number of questions to generate per API\n",
        "      call.\n",
        "\n",
        "    generation_config: dict. Configuration for text generation.\n",
        "    tool_config: dict. Configuration for tool usage.\n",
        "\n",
        "  Returns:\n",
        "    A list of dictionaries, where each dictionary represents a generated\n",
        "      question.\n",
        "  \"\"\"\n",
        "  questions = []\n",
        "  current_prompt = [file_ref, base_prompt]\n",
        "\n",
        "  for prompt_index in range(total_questions // questions_per_prompt):\n",
        "    # Append instruction to avoid repeating questions in subsequent calls after\n",
        "    # the first prompt\n",
        "    if prompt_index > 0:\n",
        "      current_prompt.append(\n",
        "          f\"{base_prompt} Make sure NOT to generate questions which are \"\n",
        "          f\"already present above.\"\n",
        "      )\n",
        "\n",
        "    per_prompt_questions = call_model_and_extract(\n",
        "        chat_model,\n",
        "        current_prompt,\n",
        "        question_type_function_name,\n",
        "        generation_config=generation_config,\n",
        "        tool_config=tool_config,\n",
        "    )\n",
        "\n",
        "    print(f\"Generated {len(per_prompt_questions)} questions in prompt {prompt_index+1}\")\n",
        "\n",
        "    questions.extend(per_prompt_questions)\n",
        "    formatted_text = format_items_for_prompt(per_prompt_questions, question_type,model_name)\n",
        "    current_prompt = [formatted_text]\n",
        "\n",
        "\n",
        "  return questions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB6uAPVycbkP"
      },
      "source": [
        "### Define the base prompt to generate multiple choice questions\n",
        "\n",
        "**Note:** `num_mcq_per_prompt` should always evenly divide total_mcq_questions.\n",
        "This ensures that we can generate the exact number of total questions\n",
        "without any remainder. In this case, `300 ÷ 30 = 10`, which is a whole number.\n",
        "If these values are changed, make sure to maintain this relationship."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_SG4e-20QyO"
      },
      "outputs": [],
      "source": [
        "total_mcq_questions = 100\n",
        "num_mcq_per_prompt = 20\n",
        "num_mcq_per_intervention = num_mcq_per_prompt // 5\n",
        "\n",
        "mcq_base_prompt = f\"\"\"\n",
        "Prompt:\n",
        "\n",
        "Task: Generate {num_mcq_per_prompt} multiple choice questions based on the provided extracted text corpus.\n",
        "\n",
        "Guidelines:\n",
        "\n",
        "Paraphrase: Avoid direct quotes and rephrase the text to create unique questions.\n",
        "Question Structure: Ensure each question is clear and concise, avoiding ambiguity.\n",
        "Answer Choices:\n",
        "  1. Provide at least five answer choices (A, B, C, D, E) for each question.\n",
        "  2. Make all options plausible, but only one correct.\n",
        "  3. Avoid using \"All of the above\" or \"None of the above\" as options.\n",
        "Correct Answer: Clearly indicate the correct answer as option from (A, B, C, D, E) for each question.\n",
        "\n",
        "Question Balance: Each intervention should have exactly {num_mcq_per_intervention} questions not less or more.\n",
        "\n",
        "1. Nutritional interventions\n",
        "2. Maternal and fetal assessment\n",
        "3. Preventive measures\n",
        "4. Interventional measures for common physiological symptoms\n",
        "5. Health systems interventions\n",
        "\n",
        "Example:\n",
        "\n",
        "Intervention: Nutritional interventions (Intervention should be one from above 5 mentioned intervention topics)\n",
        "Question: In non-endemic areas, when is preventive anthelminthic treatment recommended for pregnant women?\n",
        "Choices:\n",
        "A. During the first trimester\n",
        "B. Throughout pregnancy\n",
        "C. During the second trimester\n",
        "D. During the second and third trimesters\n",
        "E. During the third trimester\n",
        "Correct Answer: D\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(mcq_base_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1IScLu7Skih"
      },
      "source": [
        "### Function calling\n",
        "\n",
        "Function calling enhances the reliability of generative models by enabling them to produce [structured data outputs](). This is achieved by leveraging the API's function calling feature to define a strict schema, which ensures robust and predictable outputs.\n",
        "\n",
        "With function calling your function and its parameters are described to the API as a `genai.protos.FunctionDeclaration`.\n",
        "\n",
        "In most basic cases the SDK converts function parameter type annotations to a format the API understands `genai.protos.FunctionDeclaration`. It's better to define them explicitly wherever possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQazYEkV1WkA"
      },
      "source": [
        "#### MCQ single question schema\n",
        "\n",
        "This schema defines the structure for a multiple-choice question (MCQ) format.   \n",
        "Each MCQ encompasses certain elements:\n",
        "\n",
        "* Context: A concise description of the relevant intervention or topic.\n",
        "* Query: The heart of the question itself.\n",
        "* Choices: Five distinct answer options, labeled A through E.\n",
        "* Solution: The definitive answer to the query.\n",
        "\n",
        "All fields are required and stored as text. This standardized format ensures consistency when generating or processing MCQs, making it easier to generate, validate, or work with question data in your application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6wlvj-_Sowa"
      },
      "outputs": [],
      "source": [
        "mcq_schema = genai.protos.Schema(\n",
        "    type = genai.protos.Type.OBJECT,\n",
        "    properties = {\n",
        "        'Intervention':  genai.protos.Schema(type=genai.protos.Type.STRING),\n",
        "        'Question':  genai.protos.Schema(type=genai.protos.Type.STRING),\n",
        "        'A': genai.protos.Schema(type=genai.protos.Type.STRING),\n",
        "        'B': genai.protos.Schema(type=genai.protos.Type.STRING),\n",
        "        'C': genai.protos.Schema(type=genai.protos.Type.STRING),\n",
        "        'D': genai.protos.Schema(type=genai.protos.Type.STRING),\n",
        "        'E': genai.protos.Schema(type=genai.protos.Type.STRING),\n",
        "        'Choice': genai.protos.Schema(type=genai.protos.Type.STRING)\n",
        "    },\n",
        "    required=['Intervention', 'Question', 'A', 'B', 'C', 'D', 'E', 'Choice']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMrdsYgM1uBY"
      },
      "source": [
        "#### Array Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ub4vkkkKn-i"
      },
      "outputs": [],
      "source": [
        "mcqs_schema = genai.protos.Schema(\n",
        "    type=genai.protos.Type.ARRAY,\n",
        "    items=mcq_schema\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ_pF6vn10a9"
      },
      "source": [
        "#### Define as function declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL2iHESZKvcI"
      },
      "outputs": [],
      "source": [
        "mcq_database = genai.protos.FunctionDeclaration(\n",
        "    name=\"mcq_database\",\n",
        "    description=textwrap.dedent(\"\"\"\\\n",
        "        Adds interventions and questions with multiple choices and its answers to the database.\n",
        "        \"\"\"),\n",
        "    parameters=genai.protos.Schema(\n",
        "        type=genai.protos.Type.OBJECT,\n",
        "        properties = {\n",
        "            'multiple_choice_questions': mcqs_schema,\n",
        "        }\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NC5WeyL18if"
      },
      "source": [
        "### Define the model with the function declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4zfdYtnLhFT"
      },
      "outputs": [],
      "source": [
        "model = model_def(model_name, safety_settings, tools=[mcq_database])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRZrH5P2zpJI"
      },
      "source": [
        "### Create a chat session\n",
        "\n",
        "The Gemini API enables you to have freeform conversations across multiple turns. The `ChatSession` class will store the conversation history for multi-turn interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nWd3GBGpmHH"
      },
      "outputs": [],
      "source": [
        "chat = model.start_chat(history=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4ol_XNimb8P"
      },
      "source": [
        "### Generate text in chat session\n",
        "\n",
        "\n",
        "This code generates a series of multiple-choice questions (MCQs).\n",
        "\n",
        "1. The code generates MCQs, aiming for a specified total number. It uses a base prompt and a file reference as initial input.\n",
        "\n",
        "2. In each iteration, it calls an Gemini model to generate questions, extracts the structured data, and adds it to a list. To avoid repetition, it includes previously generated questions in subsequent prompts.\n",
        "\n",
        "3. It also handles potential errors during the API requests, ensuring the process continues even if individual calls fail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fbHtakWdUKT"
      },
      "outputs": [],
      "source": [
        "mcq_questions = generate_questions(\n",
        "    chat_model=chat,\n",
        "    file_ref=file_ref,\n",
        "    base_prompt=mcq_base_prompt,\n",
        "    total_questions=total_mcq_questions,\n",
        "    question_type=\"mcq\",\n",
        "    question_type_function_name=\"multiple_choice_questions\",\n",
        "    questions_per_prompt=num_mcq_per_prompt,\n",
        "    generation_config=generation_config,\n",
        "    tool_config={\"function_calling_config\": {\"mode\": \"ANY\"}},\n",
        "    model_name=\"gemini\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHXJhiAr1miY"
      },
      "source": [
        "### Convert the MCQ's into DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oRLUbq3cL_Q"
      },
      "outputs": [],
      "source": [
        "mcq_questions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6LPY9HOPktx"
      },
      "outputs": [],
      "source": [
        "mcq_df = pd.DataFrame(mcq_questions, columns=[\"Intervention\", \"Question\", \"A\", \"B\", \"C\", \"D\", \"E\", \"Choice\"])\n",
        "mcq_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMlay7bwPjLX"
      },
      "outputs": [],
      "source": [
        "# Write the dataframe to a CSV file\n",
        "mcq_df.to_csv(\"mcq_questions.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84YtmsGQi1Yv"
      },
      "source": [
        "## Generate set of short-form answer questions\n",
        "\n",
        "The questions created from the World Health Organization's guidelines are divided equally among the five main categories of recommendations for pregnancy care:\n",
        "\n",
        "1. Nutritional interventions\n",
        "2. Maternal and fetal assessment\n",
        "3. Preventive measures\n",
        "4. Interventions for common physiological symptoms\n",
        "5. Health systems interventions to improve the utilization and quality of antenatal care\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BecV2uVVmWW0"
      },
      "source": [
        "### Define the base prompt for the task\n",
        "\n",
        "**Note:** num_mcq_per_prompt should always evenly divide total_mcq_questions.\n",
        "This ensures that we can generate the exact number of total questions\n",
        "without any remainder. In this case, `300 ÷ 30 = 10`, which is a whole number.\n",
        "If these values are changed, make sure to maintain this relationship."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbcts3PpmSWI"
      },
      "outputs": [],
      "source": [
        "total_number_sqa_questions = 100\n",
        "num_sqa_per_prompt = 20\n",
        "num_sqa_per_intervention = num_sqa_per_prompt // 5\n",
        "\n",
        "sqa_base_prompt= f\"\"\"\n",
        "Please generate {num_sqa_per_prompt} short answer questions from extracted_text corpus provided.\n",
        "Answer should not be less than 20 words.\n",
        "While generating text please make sure to paraphrase the text from the original text.\n",
        "\n",
        "We have 5 intervention topics:\n",
        "1. Nutritional interventions\n",
        "2. Maternal and fetal assessment\n",
        "3. Preventive measures\n",
        "4. Interventions from common physiological symptoms\n",
        "5. Health systems interventions to improve the utilization and quality of antenatal care.\n",
        "\n",
        "Each should have exactly {num_sqa_per_intervention} question not less or more.\n",
        "Mention the intervention topic from the above 5\n",
        "Mention the question\n",
        "Mention the answer\n",
        "\n",
        "For Example each Short Questions Answer format should look like this:\n",
        "\n",
        "Intervention: Interventions from common physiological symptoms\n",
        "Question: What advice can help manage varicose veins during pregnancy?\n",
        "Answer: Women should be advised to wear compression stockings as prescribed by their healthcare provider and to avoid standing or sitting for long periods.\n",
        "\"\"\"\n",
        "\n",
        "print(sqa_base_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUnZePVg9IYe"
      },
      "source": [
        "### Function calling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wW4Kffr9jp9"
      },
      "source": [
        "#### Single Short Question Answer Schema\n",
        "\n",
        "This schema defines the structure for short answer questions (SQAs). It includes the following fields:\n",
        "\n",
        "* Intervention: The topic or subject matter related to the question.\n",
        "* Question: The actual short answer question.\n",
        "* Answer: The correct answer to the question.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yh9f6Te9JiS"
      },
      "outputs": [],
      "source": [
        "sqa = genai.protos.Schema(\n",
        "    type = genai.protos.Type.OBJECT,\n",
        "    properties = {\n",
        "        'Intervention':  genai.protos.Schema(type=genai.protos.Type.STRING),\n",
        "        'Question':  genai.protos.Schema(type=genai.protos.Type.STRING),\n",
        "        'Answer': genai.protos.Schema(type=genai.protos.Type.STRING)\n",
        "    },\n",
        "    required=['Intervention', 'Question', 'Answer']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbK3yqQR9ses"
      },
      "source": [
        "#### Now declare each SQA in `ARRAY` type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IryQ5zS9z52"
      },
      "outputs": [],
      "source": [
        "sqas = genai.protos.Schema(\n",
        "    type=genai.protos.Type.ARRAY,\n",
        "    items=sqa\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EQKd2gMpmhM"
      },
      "source": [
        "#### Function Declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3oEg8XL939-"
      },
      "outputs": [],
      "source": [
        "sqa_database = genai.protos.FunctionDeclaration(\n",
        "    name=\"sqa_database\",\n",
        "    description=textwrap.dedent(\"\"\"\\\n",
        "        Adds interventions, questions and its short answers to the database.\n",
        "        \"\"\"),\n",
        "    parameters=genai.protos.Schema(\n",
        "        type=genai.protos.Type.OBJECT,\n",
        "        properties = {\n",
        "            'short_answer_questions': sqas,\n",
        "        }\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rX-RkXi-ima"
      },
      "source": [
        "### Define the model with the function declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OD3pLF5-gFX"
      },
      "outputs": [],
      "source": [
        "model = model_def(model_name, safety_settings, tools=[sqa_database])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnaeTUwqYCIb"
      },
      "source": [
        "### Create a chat session\n",
        "\n",
        "The Gemini API enables you to have freeform conversations across multiple turns. The ChatSession class will store the conversation history for multi-turn interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf7J0BN1YCWO"
      },
      "outputs": [],
      "source": [
        "chat = model.start_chat(history=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgae_PEjrzNF"
      },
      "source": [
        "### Generate text in chat session\n",
        "\n",
        "\n",
        "This code generates a series of Short Question Answers (SQAs).\n",
        "\n",
        "1. The code generates SQAs, aiming for a specified total number. It uses a base prompt and a file reference as initial input.\n",
        "\n",
        "2. In each iteration, it calls an Gemini model to generate questions, extracts the structured data, and adds it to a list. To avoid repetition, it includes previously generated questions in subsequent prompts.\n",
        "\n",
        "3. It also handles potential errors during the API requests, ensuring the process continues even if individual calls fail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8oibnOGS7j0"
      },
      "outputs": [],
      "source": [
        "sqa_questions = generate_questions(\n",
        "    chat_model=chat,\n",
        "    file_ref=file_ref,\n",
        "    base_prompt=sqa_base_prompt,\n",
        "    total_questions=total_number_sqa_questions,\n",
        "    question_type_function_name=\"short_answer_questions\",\n",
        "    question_type=\"sqa\",\n",
        "    questions_per_prompt=num_sqa_per_prompt,\n",
        "    generation_config=generation_config,\n",
        "    tool_config={\"function_calling_config\": {\"mode\": \"ANY\"}},\n",
        "    model_name = 'gemini'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ54ldlns3tI"
      },
      "source": [
        "### Convert to DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjHRjeaU4hyO"
      },
      "outputs": [],
      "source": [
        "sqa_df = pd.DataFrame(sqa_questions)\n",
        "sqa_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJv2Z0BJQFov"
      },
      "outputs": [],
      "source": [
        "# Write the dataframe to a CSV file\n",
        "sqa_df.to_csv(\"sqa_questions.csv\", index=False)"
      ]
    }
  ]
}